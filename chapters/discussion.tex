\chapter{Discussion}
\label{discussion}

\section{Examining the Kansas Signatures}
\label{discussion:kssigs}

The Kansas signatures extracted from the k-means clusters (\autoref{fig:KScropsigs}) were not as variable as I was expecting. Based on initial testing results, presented in \autoref{appendix:testing:r3} beginning on \autopageref{appendix:testing:r3}, I expected some of the cluster signatures would be much ``stranger'' looking. Aside from the Soy\_1 signature and perhaps the Corn\_1 signature to some degree, however, the cluster signatures were fairly typical in appearance. The Sorghum\_1 signature appears more or less as expected over the TSI's date range, but does seem to be missing the late-season downslope.

Using the k-means algorithm to cluster each crop's pixels does not seem to have adequately captured the variability in the crop signatures as labeled by the CDL to allow the fit algorithm to match the CDL classification. That is, perhaps k-means separates pixels that would have similar RMSE values when fit with the same crop reference signature. Future research might want to consider clustering based on the RMSE value of each pixel to the others; pixels with low RMSE values when compared to one another would be grouped together, as they are likely transformations of the same base temporal signature. Pixels with considerably different RMSE values would suggest different base temporal signatures.

Despite the fact the k-means might not capture similarity in the same way as the fit algorithm, it is worth noting that the k-means clusters generally do not divide fields: each pixel in a field is assigned the same cluster (\autoref{map:KSclusters}). This result shows that all pixels in a field typically have comparable signatures, at least using k-means as a measure of similarity. This result is further confirmation of the hypothesis that pixels of a crop grown under the same conditions should have the same development and therefore the same temporal signature.

\section{Breakdown of the Kansas Classification}
\label{discussion:ksclassification}

The initial verification of the Kansas reference signatures, done by classifying the TSI of the Kansas study area, demonstrates the method performs well when used to match the original source data (\autoref{table:ksresults}). The 84.4 percent overall accuracy and kappa value of 0.76 are well within the range considered acceptable for this type of classification, especially given the CDL's published accuracy of 88.4 percent (to which this classification is compared).

Some confusion between corn and soy, as well as soy and ``other,'' pulled down the overall accuracy, as well as the producer and user accuracies of each of those classes. The similarities between the corn and soy signatures may cause late corn and early soy to be confused if the range of the offset of the maximum DOY of the reference signatures, $tshift$, allows overlap between the two. I believe much of the soy and ``other'' confusion was due to the Soy\_1 reference signature (\autoref{fig:KScropsigs}). Examining the CDL classes of the best fit pixels in the RMSE raster showed a number of grassland pixels were well fit by that particular signature. However, omitting the Soy\_1 signature seemed to allow one of the corn signatures to take many of the soy pixels, which I find strange due to the peculiar shape of the Soy\_1 signature. In fact, the signature does not match the traditional soy signature, and makes me question the validity of the CDL in this case.

Classifying sorghum did not seem to be very effective; only two of the 18 sorghum pixels were accurately identified. The sorghum RMSE raster had the lowest threshold value at 450, but increasing the threshold only caused greater class confusion. Omitting the signature entirely had a slight negative effect on the overall accuracy: the ``other'' pixels it misclassifies would otherwise be misclassified as corn or soy. Adding another 16-day composite to the TSI to capture the tail-end of the sorghum signature may have alleviated the confusion. However, interference between winter crop germination and summer crop signatures would likely have resulted in worse classification errors (for more information see the ``end-of-year bump'' discussion in \autoref{appendix:testing:r4} beginning on \autopageref{appendix:testing:r4}).

Moreover, the similarities between sorghum and soy signatures might cause confusion if the sorghum threshold were to be raised. However, it is also possible that the sorghum pixels were not accurately identified by the CDL. The CDL does not have clearly-delineated sorghum fields, but rather a mix of soy and sorghum. Thus, it appears that the USDA’s classification method is unable to accurately differentiate between soy and sorghum in some cases. Consequently, sorghum pixels may have been included in the soy clusters, and vise versa. This would mix the temporal signatures and cause confusion in the results. Of course, the soy and sorghum signatures are rather similar in appearance, and it might be that my method is unable to distinguish them from one another. Even if the CDL has confusion between the two, this may still be case. Due to the low number of sorghum pixels, the validity of any conclusions about classifying this particular crop is questionable. A larger sample size and more testing are required.

The strange soy signature mentioned above, the sorghum problems, and additional CDL errors identified in preliminary testing (see \autoref{appendix:testing:r3:results} on \autopageref{appendix:testing:r3:results}) make me distrust the CDL as a ground truth dataset. Lacking alternative data, I have proceeded through this investigation assuming the CDL is correct. The classification results using clustering do show that my tools work, as they were able to reproduce the CDL with decent accuracy. Some errors in my classification may be due to errors in the CDL. Even so, my accuracy is not necessarily representative of the method's performance; determining exactly how well the tools perform is difficult without more accurate ground truth data. I believe a ground truth without errors would allow classification using a single reference signature per crop. Clustering to find multiple crop signatures was necessary to work around the less reliable CDL data.\footnote{My experience working with the CDL has revealed a number of problems. Additionally, others have expressed reservations to me about the published accuracies. That said, the CDL is a fantastically ambitious---and free---dataset, and I do not want to suggest that the USDA has failed. Even if the data have problems, the USDA are classifying the entire continental United States, which has extremely variable conditions and an incredible variety of land covers. That the USDA can achieve any reasonable classification at all is impressive.} 


\section{The Pellegrini Classification and Class Confusion}

As shown in \autoref{map:ARclassification}, classifying the Argentina TSI with the reference signatures from the k-means clustering of the Kansas TSI was able to effectively separate areas of the summer crops corn, soy, sorghum, and poroto from most other land cover classes, but identified those summer crop pixels predominately as corn. While \autoref{table:ARbestresult} reflects this class confusion in the producer and user accuracies, the low sample count for corn, soy, and sorghum compared to all the other land covers deceptively skews the overall accuracy higher. \autoref{table:ARpurepxresults} is the confusion matrix for the same classification, but compared to the entire ground truth dataset, encompassing some 85,821 pixels, instead of the rather limited 378 random sample points. While the overall accuracy actually improves slightly with this new reference dataset, it must be noted that neither of these accuracy assessments is able to account for errors resulting from mixels. However, the increased number of samples better demonstrates the significant corn-soy confusion. For instance, of the 6,621 pixels classified as corn, 2,076 are soy. Errors of omission are also more prominent: 2,234 of the 5,706 corn pixels were left as ``other.'' A similar proportion of soy pixels were also classified ``other.'' Raising the RMSE thresholds on the RMSE rasters to decrease these omissions only increased the errors of commission, confusing ``other'' pixels for crops. The low kappa statistic of both accuracy assessments, 0.54 and 0.51 respectively, is reflective of the poor accuracies within the summer crops.

\begin{sstable}
  \centering
  \caption[Summer 2014 Pellegrini Best Classification Accuracy Checked Against All Pure Pixels]{Summer 2014 Pellegrini Best Classification Accuracy\\Checked Against All Pure Pixels}
  \label{table:ARpurepxresults}
  \begin{tabu}{rrrrrrrl}
    \toprule
     & & \multicolumn{4}{c}{\textbf{Reference Data}} & & \\
     &  & Corn & Soy & Sorghum & Other & Total & User Acc. \\
    \midrule
    \multirow{4}{*}{\rotatebox{90}{\textbf{Classified}}} & Corn & 3283 & 2076 & 61 & 1201 & 6621 & 49.58\% \\
     & Soy & 189 & 313 & 36 & 458 & 996 & 31.43\% \\
     & Sorghum & 0 & 0 & 0 & 0 & 0 & 0.00\% \\
     & Other & 2234 & 1523 & 60 & 74387 & 78204 & 95.12\% \\
     & Total & 5706 & 3912 & 157 & 76046 & 85821 &  \\
     & Producer Acc. & 57.54\% & 8.00\% & 0.00\% & 97.82\% &  &  \\
    \multicolumn{8}{r}{Overall Accuracy: 90.87\%} \\
    \multicolumn{8}{r}{Kappa: 0.51} \\
    \bottomrule
  \end{tabu}
\end{sstable}

Considering that the ``other'' pixels contain a number of different classes, I found the frequency of corn and soy classifications within each of the ``other'' land cover classes. The results of this analysis (\autoref{table:ARotherconfusion}) show that the main sources of confusion were, from greatest to least, the true other land cover class, pasture, and poroto. However, when finding the percent of the land cover class pixels that were confused, poroto leads with over 26 percent of its pixels confused for either corn or soy. This confusion, and the confusion of some pasture areas as well, does make some sense, as these land covers, soy, and corn are all planted early in the year and reach peak maturation during the summer months. Depending on the type of pasture, it may or may not grow back after cutting; if it does not, the temporal signature may bear some resemblance to corn or soy.\footnote{I heard a few names for the different types of pasture grasses that were being grown in the area, but I believe the two most prevalent are known locally as \textit{\textspanish{gatom pani}} and \textit{\textspanish{grama}}. I have not been able to determine if \textit{\textspanish{gatom pani}} has an English name; it is possible that I did not get the correct spelling. Blue Grama Grass (\textit{Bouteloua gracilis}) is a common forage grass native to North America, though I am unsure if it is the same plant grown in Pellegrini. I did not learn much about any other pasture grasses cultivated in the area, or about typical harvesting practices.}

\begin{table}[b]
  \begin{Spacing}{1.0}
  \centering
  \caption{Pellegrini Corn and Soy Confusion with ``Other'' Land Cover Classes}
  \label{table:ARotherconfusion}
  \begin{tabu}{X[0.6,m,c]X[0.5,m,c]|X[1,m,c]X[0.55,m,c]|X[1,m,c]X[0.55,m,c]}
    \toprule
    \textbf{Land Cover} & \textbf{Total Pixels} & \textbf{Confused as Corn} & \textbf{Percent of Total} & \textbf{Confused as Soy} & \textbf{Percent of Total} \\
    \midrule
    Forested & 63,978 & 194 & 0.30 & 26 & 0.04 \\
    Other & 5,393 & 306 & 5.67 & 322 & 5.97 \\
    Pasture & 5,252 & 396 & 7.54 & 50 & 0.95 \\
    Poroto & 1,369 & 303 & 22.13 & 59 & 4.31 \\
    Nothing & 485 & 2 & 0.41 & 1 & 0.21 \\
    \bottomrule
  \end{tabu}
  \end{Spacing}
\end{table}

From their appearance in Landsat imagery, the main locations where the other class was confused for corn and soy seem to be bare earth, possibly due to high soil salinity. The areas have low-to-moderate reflectivity in the visual bands, high reflectivity in the mid-infrared, and low reflectivity in the near-infrared, and do not exhibit much change over time. However, the plots of a random sampling of pixels from the TSI show temporal signatures like that in \autoref{fig:ARweirdsig}.
I am currently unable to explain what is in these areas or why they are confused for corn and soy. I believe there may be some sort of summer grass cover or other seasonal vegetation, but it is hard to explain the lack of near-infrared reflectance as observed in Landsat images from multiple dates throughout the summer. Perhaps some standing water coinciding with Landsat observations reduces the near-infrared reflectance, but the presence of vegetation results in high VI values on other dates, which are recorded in the MODIS composites.

\begin{ssfigure}
  \centering
  \input{plots/strangepoint1.pgf}
  \caption[Signature of an Unknown Non-Crop Pixel Confused for Corn and Soy in Pellegrini]{Signature of an Unknown Non-Crop Pixel\\Confused for Corn and Soy in Pellegrini}
  \label{fig:ARweirdsig}
\end{ssfigure}


\section{Clustering Pellegrini}

To further examine the class confusion in Pellegrini, I used the same k-means clustering as in Kansas to identify the three main signatures for each of the eight land cover classes in the ground truth dataset. The extracted corn, soy, and sorghum signatures are shown in \autoref{fig:ARcropsigs}, while the poroto and pasture signatures are in \autoref{fig:ARporotopasturesigs}. The forest, nothing, and other signatures are shown in \autoref{fig:ARothersigs}.

\begin{ssfigure}
  \centering
  \input{plots/cropsigsAR.pgf}
  \caption{Corn, Soy, and Sorghum Signatures Extracted from the Pellegrini TSI}
  \medskip
  \small
  These are the signatures from the k-means crop clusters found in the Pellegrini TSI. While some strange exceptions like Soy\_3, Sorghum\_1, and Sorghum\_3 deviate from the rest, the overwhelming similarities between signatures of different crops is striking. Unlike the crop signatures from Kansas, Pellegrini's crops are not temporally separated, but peak almost simultaneously.
  \label{fig:ARcropsigs}
\end{ssfigure}

\begin{ssfigure}
  \centering
  \input{plots/poroto_pasture_sigsAR.pgf}
  \caption{Poroto and Pasture Signatures Extracted from the Pellegrini TSI}
  \label{fig:ARporotopasturesigs}
\end{ssfigure}

\begin{ssfigure}
  \centering
  \input{plots/othersigsAR.pgf}
  \caption[Forested, ``Nothing,'' and ``Other'' Signatures Extracted from the Pellegrini TSI]{Forested, ``Nothing,'' and ``Other'' Signatures\\Extracted from the Pellegrini TSI}
  \label{fig:ARothersigs}
\end{ssfigure}

The cause of the corn-soy confusion is immediately visible: both crops peak around the same date. I did not expect this result, as the typical planting dates I collected suggest soy should peak earlier than corn.\footnote{Even if soy had peaked earlier than corn, I would not have been able to achieve an accurate classification with the current tools, as corn-before-soy is a key assumption. That is, a single $tshift$ parameter is specified for all reference signatures. It would be possible to rewrite the tool to allow different $tshift$ values for different signatures, which might have sufficed if soy did peak before corn, but does not help when the peaks are coincident.} However, I also gathered that precipitation was the limiting factor in planting \autocite[confirmed by][]{sacks2010crop}, and often farmers will wait until a certain amount of rain has fallen before planting. In fact, I was told the rains in the 2014 growing season were quite late, and on multiple occasions farmers told me they had planted a field late due to lack of rain. While I must admit I am not a farmer and do not know this for certain, the reason for waiting did not seem to be out of concern for plant health, in that too early of planting would negatively impact the crop's health. Rather, the rationale seemed primarily economic: farmers do not want to pay to plant crops that will not grow if the rains never come.

I also observed fields of corn, soy, sorghum, and poroto in many different states of maturation, from the early, barely-germinated stage to the late, fruit-bearing stage. To me, the broad range of development is likely because water is the only limiting factor defining the typical planting dates. That is, the threat of changing temperatures does not affect planting decisions like in Kansas, and farmers have much more flexibility (this flexibility has been observed in other regions per \textcite{sacks2010crop}). Extremely early plantings can capitalize on early rains at the risk of crop failure. The rest of the plantings, both early and late, may germinate at different times, but should mature around the same date, their development being limited by the available water. My method should be able to accommodate an increased range of planting and maturation dates with looser bounds on the $tshift$ parameter, allowing the reference signatures more freedom to temporally align themselves with the pixel values. However, there is one problem with that idea: my method handles similar-looking signatures, like corn and soy, by making the assumption that each should peak within a different time range. In Kansas, corn peaks before soy, and as such they can be differentiated. Planting dates for Argentina as a whole also suggest corn should peak before soy. However, in Pellegrini specifically, the weather conditions and consequent agricultural practices do not allow this assumption to be met. When both crops peak about the same date, their temporal signatures are not significantly different, leading to the class confusion exhibited.

I believe higher temporal resolution data might create more detailed temporal signatures, which could allow for more difference to be detected between different crops. Combining such data with certain noise-filtering methods (i.e.,\ the TSF wavelet filtering) may allow signatures to be smoothed in ways that might accentuate differences between similar crops like corn and soy (see \textcite{doraiswamy2006improved} and \textcite{sakamoto2010a-two-step} for examples of higher temporal resolution data and filtering).


\section{More Ideas for Future Research}

Given the practical constraints of this project, many aspects of this approach to classification have gone largely or completely untested, and some could have a profound impact on the classification accuracy and potential use-cases. I am sure that I have not thought of everything that could be tested or improved, but I have compiled the following list of ideas to help future researchers working with these tools, broken down by each step of the workflow.

\subsection{Temporal Signature Generation}

How does spatial and temporal variation in the sources used to generate a temporal signature (e.g.\ multiple study sites and multiple sample years, respectively) affect the accuracy of classifications produced with that signature? I initially tried to investigate this question, but my findings were largely inconclusive due to numerous problems (my initial testing is described in \autoref{appendix:testing:r1:results} on \autopageref{appendix:testing:r1:results}). Now that I have refined the method somewhat, this question could be revisited. My hypothesis remains the same: the greater the variations averaged together to create a reference signature, the further that signature gets from an ideal reference which can be transformed effectively to fit pixels of that crop. That is, extracting signatures from a large geographical area, or across multiple years, will only create reference signatures that are less ``true'' than those created with less variation, spatially or temporally.

However, could another method of averaging the pixel signatures used to create a reference signature be less prone to these variations? In addition to the arithmetic mean of each dates' values, as presented in the methods, I have tried something like “fit averaging.” The time shift and horizontal and vertical differences between two signatures can be found using the TSF equations. Dividing each of those transformations by two and applying them to one of the original signatures theoretically creates a new signature halfway between the two original signatures. However, in practice, this resulted in strange curve forms when the signatures averaged were not very similar. It may be that this method deserves a second test using the refined sample points, which all have similar shapes, as this approach may be particularly sensitive to outliers. It could also be that the program I used for this function has errors. Similarly, it would be worthwhile to test the effect of a geometric rather than arithmetic mean of the sampled pixels’ values. Such a mean is less sensitive to outliers and may provide truer signatures. Additionally, other methods mat be more effective when generating reference signatures from disparate data. 

\subsection{Mixels}

It seems to me that many methods could be more effective at negating the effects of mixels rather than my simple elimination. For instance, perhaps some method of signature unmixing could allow for sub-pixel classification. However, even using the basic elimination process, a few things should be tested. First, what is the purity threshold at which a mixel is too mixed? That is, what percentage of a whole pixel can still be accurately classified? I arbitrarily chose to eliminate all pixels less that 98 percent of a whole MODIS pixels. I assumed the possible 2 percent mixing was low enough that it would have no negative impact. Yet, perhaps I could have chosen many more mixed pixels with a higher mix threshold, or should have eliminated all pixels, even those so minimally mixed. An effective mix threshold may not be constant, however, as all land covers may not contribute equally to a mixel's temporal signature; this is also a subject for future investigations.

Another idea to work around mixels would be to integrate this classification method with more-traditional methods that use higher spatial resolution data. For example, thresholding the RMSE rasters at a low RMSE value would identify fields that have the highest probabilities of correct classification. Those identified pixels could then be converted into training sites for use with \citeauthor{wardlow2005state-level}'s decision tree classification method, or with a multispectral classifier. Another advantage of this idea is that it would eliminate the less-than-ideal and lengthy threshold iteration.

\subsection{The Fitting Process}

As I mentioned in the methods section, the bounds chosen for the transformation parameters $tshift$, $xscale$, and $yscale$ were not thoroughly tested, and likely have a large effect on the classification accuracy. Loosening the bounds would allow a reference temporal signature to to better fit pixel signatures that are dissimilar, but could increase confusion and errors of commission. Tightening the bounds would restrict the amount of possible signature transformation. Stricter transformation may reduce confusion, but could increase errors of omission. Also unclear is whether transformation parameter bounds should be the same across all crop signatures, or if each crop should have an independent set of bounds (perhaps due to differing variability in the signatures of different crops).

Another possible optimization is changing how curve similarity is calculated between reference signatures and pixel signatures. The current approach uses an arithmetic mean to calculate the RMSE, as given in \autoref{eq:1}. Similarly, the geometric mean could be used:
\begin{equation}
\label{eq:geometricmean}
  RMSE = \Biggl[\biggl[\prod_{x\ =\ j(1),\ j(2)\ldots}^{n}\bigl(f\left(x\right)-g\left(x\right)\bigr)^{2}\biggr]^{\frac{1}{2}}\Biggr]^{\frac{1}{n}}
\end{equation}
I tried using the geometric mean for RMSE calculation in the fit algorithm, but I found it was too insensitive. RMSE values were unrealistically low. This mean seemed to allow reference signatures to fit to pixels that should not be classified (they are ``other'') in a similar threshold range to pixels which are legitimate. However, given subsequent optimizations to the overall classification process, formally testing this mean would be of value. The Find Fit Tool already has an option to use a geometric mean instead of an arithmetic mean, so testing would not require any programming changes.

A method of curve comparison beyond RMSE may be better suited to this analysis. I used RMSE because it was shown to work in \textcite{brown2007multitemporal} and \textcite{sakamoto2010a-two-step}. However, another measure of curve likeness might perform better. I do not know what that method might be; ideas I want to investigate include Procrustes distance, dynamic time warping, and Frechet distance, though these may not all be applicable to this problem.

Also worth revisiting is the question of which VI performs best, or if there are certain conditions that favor specific VIs over others. My initial testing showed that NDVI worked better than EVI, but many improvements have been made to the method and program code since that testing was completed. Moreover, many other VIs could be tested for use with this method, and some may allow use cases outside agricultural classification. Similarly, this method may not be limited to use with just VIs; data other than VIs could allow for applications of the method beyond what I can imagine. What if the method were used multi-dimensionally, with multiple bands? Or with different data sources, i.e.\ higher spatial resolution data?

\subsection{Thresholding and Classification}

The thresholding and classification step is the part of this method I dislike the most. My original intention was to create a method for classifying crops without ground truth. Due to the complex thresholding process, optimal classifications are impossible to create without iterative accuracy assessments, which require ground truth. I hope that future research may devise a way to remove this limitation from the optimization process. The accuracy impact of achieving the absolute optimum classification over less-optimized classification is unknown. Is not well understood is if the optimal RMSE threshold varies between crop types. The wide spread of optimal RMSE thresholds that I’ve identified across the classifications I've created suggests that the best RMSE threshold values are unique to every circumstance and quite variable. However, it is also possible that a single threshold value used across all the RMSE rasters could result in classifications of reasonable accuracies.

While I cannot draw any solid conclusions, I did make a test classification of the summer 2012 Kansas TSI using the k-means clustering discussed in \autoref{methods:sigextraction} (\autopageref{methods:sigextraction}) using ten clusters per crop, rather than three. This clustering produced thirty clusters: ten each for corn and soy, nine for wheat-soy double cropped, and one for sorghum. Processing thirty RMSE rasters with the classification/accuracy tool using just two RMSE value thresholds would theoretically take $0.35\times2^{30}$ seconds, or almost twelve years, to complete on my computer. Rather than wait for that process to complete, I decided to use a single threshold across all the RMSE rasters and see what would result.

I stepped through RMSE value thresholds of 400 to 850 by increments of 50. The highest accuracy classification, 71.7 percent, occurred at with a RMSE threshold of 650 across all the RMSE raster. Omitting the wheat-containing reference signatures boosted the classification accuracy to 79.2 percent at the same 650 threshold value. A last test without the sorghum signature proved to be the highest accuracy at 81.0 percent, again with the same RMSE threshold of 650.

If a single threshold value proves to be too inaccurate, finding a faster way to iterate through the RMSE thresholds combinations is an important task. One challenge of thresholding are local maxima. In other words, the accuracy may peak with a certain threshold combination, but that peak may only be a local maximum, and a different threshold combination may allow an even higher accuracy. One idea would be to investigate numerical optimization methods that could use the range of values in a RMSE raster to calculate an optimal threshold for that raster. If optimal RMSE thresholds could be approximated numerically, then no ground truth would be needed to create the classification.
