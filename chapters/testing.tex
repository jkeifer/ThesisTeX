\chapter{A Breakdown of all Completed Testing}
\label{appendix:testing}

\section{Round 1 Testing: Initial Classifications}
\subsection*{Testing Considerations}

I began testing the toolset (explained in \autoref{appendix:tools}) with three main questions. I wanted to know how the accuracy of classification is affected by:

\begin{Spacing}{1.2}
\begin{itemize}
  \item The spatial distribution of pixels chosen to create the reference curves.
  \item The temporal distribution of pixels chosen to create the reference curves.
  \item The VI used for the classification.
\end{itemize}
\end{Spacing}

To test these factors, I chose six small study sites dispersed across Kansas, each 100 MODIS pixels square, or about 2.3 square kilometers (\autoref{fig:studysites}). Using the USDA CDL as reference, I identified areas containing a mix of corn, soy, sorghum, winter wheat,  and winter wheat/soybean double crop pixels. I did my best to distribute the study sites across the state to capture a wide variety of growing conditions. This task was surprisingly difficult, given the large extent of the state and the concomitant variation in growing conditions. The crops favored tended to change from one area to another. For example, few areas in western Kansas had little more than corn and wheat.

\begin{ssfigure}
  \centering
  \includegraphics[width=\textwidth]{Graphics/Testing/STUDYSITES.pdf}
  \caption{The Six Kansas Study Sites}
  \label{fig:studysites}
\end{ssfigure}

Next, I assembled a TSI for all of Kansas from summer 2012 MODIS 16-day NDVI composite imagery, as described in \autoref{buildingTSIs} on \autopageref{buildingTSIs}. I also used the same procedure to create a Kansas TSI using the MODIS EVI composites. I clipped smaller, individual NDIV and EVI TSIs for each of the six study sites from the larger Kansas image.

Within each study site, I found four to eight TSI pixels of corn, soy, sorghum, winter wheat, and winter wheat/soy double crop. I took care to choose only pixels in the center of fields, under the assumption such pixels would be more representative of each crop’s true temporal signature. On each chosen pixel, I digitized a vector point feature (see\autoref{fig:refpoints} for an example from study site 1). I separated the points in separate shapefiles for each crop in each study site. I used these shapefiles as inputs to the RSG. Both NDVI and EVI reference temporal signatures were extracted in each study site TSI. Averaging the six extracted NDVI reference signatures for each crop gave me NDVI reference temporal signatures averaged across all six study sites. I also did the same averaging of the EVI signatures.

\begin{ssfigure}
  \centering
  \includegraphics[width=.7\textwidth]{Graphics/Testing/clip1_30mCDL_smpl_old.pdf}
  \caption{Points Marking Pixels Used to Extract Reference Signatures in Study Site 1}
  \label{fig:refpoints}
\end{ssfigure}

I created seven classifications of the EVI TSIs for each of the study sites: one classification using the study site's own reference signatures, five classifications of the study site, each one using the reference signatures derived from one of the other study sites, and one classification using the mean reference signatures averaged across all of the study sites. I repeated this procedure to produce an additional seven classifications of the NDVI TSIs of each study site. For all of these classifications I used only the corn, soy, and winter wheat reference signatures.

By assessing the accuracy of these classifications I could test two of my initial questions: how the spatial distribution of pixels used to construct reference signatures affects the accuracy of classification, and which of the VIs had better performance. For the spatial distribution testing, I determined there could be four possible outcomes:

\begin{enumerate}
\begin{Spacing}{1.2}
  \item \textit{Reference signatures are usable between study sites, but averaging multiple sites increases classification accuracy.}\\
  Classification accuracies will be largely independent of the reference signature set used. However, the mean reference signatures will produce a higher classification accuracy than those derived from single study sites.
  \item \textit{Reference signatures are usable between study sites, but averaging multiple sites decreases classification accuracy.}\\
  Classification accuracies will be largely independent of the reference signature set used. However, the mean reference signatures will produce a lower classification accuracy than those derived from single study sites.
  \item \textit{Reference signatures are not useable between study sites.}\\
  Classifications using reference signatures from different study sites will have consistently lower accuracies than the study site's own reference signatures. The mean reference signatures will perform somewhere between those of the study site in question and those of the other study sites.
  \item \textit{Spatial distribution has no effect.}\\
  The classification accuracies will be relatively consistent no matter which reference signature set is used.
\end{Spacing}
\end{enumerate}

I initially hypothesized that averaging signatures across multiple sites would decrease the ``truthfulness'' of the reference signatures. That is, geographical discrepancies in season start date, maximum VI intensity, and/or season length would skew the resultant mean reference signature away from a ``true'' reference signature. Such skewing should negatively affect classification accuracy. I therefore expected outcome two.

As for my third question (about temporal variation in the construction of reference temporal signatures), I decided to wait and see what results I got from these tests. If I found a wider spatial distribution of pixels negatively affected the classification accuracies, then I could reasonably conclude adding a temporal component would have a similar outcome: temporal variance, like spatial variance, would introduce discrepancies in season start date, maximum VI intensity, and/or season length.


\subsection*{Round 1 Results and Discussion}
\label{appendix:testing:r1:results}

The accuracies of the fourteen classifications testing the spatial distribution of reference signatures and the performance of the two MODIS VIs is shown in \autoref{table:round1results}. Some clear patterns are immediately obvious. First, none of the classifications had a very high degree of accuracy; study site 3 stands out with much higher accuracies than the rest. Secondly, NDVI resulted in a higher top accuracy for every study site. Thirdly, aside from study site 5 in the EVI-based classifications, each site's highest accuracy occurred when its own signatures were used for the classification. Lastly, generally all of the mean reference signature classifications are between the lowest and highest accuracies for each study site.

\begin{sstable}
  \centering
  \caption[Overall Percent Accuracy for each Round 1 Classification]{Overall Percent Accuracy for Each Round 1 Classification,\\~by Study Site (SS). Green cells indicate highest accuracy for each SS.}
  \label{table:round1results}
  \begin{tabu}{ZZZZZZZZ}
    \toprule
    \multicolumn{8}{c}{\textbf{EVI}} \\
    \midrule
    & \multicolumn{7}{c}{Reference Signatures Source} \\
    & SS 1 & SS 2 & SS 3 & SS 4 & SS 5 & SS 6 & Mean \\
    \midrule
    SS 1 & \cellcolor{LimeGreen}55.61 & & 45.34 & 54.36 & 43.83 & 49.06 & 49.63\\
    \rowcolor{light-gray}SS 2 & 53.11 & \cellcolor{LimeGreen}64.93 & 50.00 & 47.86 & 40.79 & 42.60 & 53.69 \\
    SS 3 & 73.87 & 69.40 & \cellcolor{LimeGreen}75.23 & 73.53 & 70.57 & 71.86 & 73.71 \\
    \rowcolor{light-gray}SS 4 & 50.42 & 45.54 & 49.26 & \cellcolor{LimeGreen}53.46 & 45.30 & 49.66 & 52.54 \\
    SS 5 & 42.05 & 45.62 & \cellcolor{LimeGreen}56.00 & 54.68 & 55.06 & 49.02 & 40.29 \\
    \rowcolor{light-gray}SS 6 & 47.78 & 48.66 & 38.43 & 47.53 & 41.60 & \cellcolor{LimeGreen}49.55 & 48.44 \\
    \bottomrule
    & & & & & & & \\
    & & & & & & & \\
    \toprule
    \multicolumn{8}{c}{\textbf{NDVI}} \\
    \midrule
    & \multicolumn{7}{c}{Reference Signatures Source} \\
    & SS 1 & SS 2 & SS 3 & SS 4 & SS 5 & SS 6 & Mean \\
    \midrule
    SS 1 & \cellcolor{LimeGreen}61.08 & 48.29 & 47.91 & 60.72 & 44.85 & 51.81 & 52.75 \\
    \rowcolor{light-gray}SS 2 & 56.08 & \cellcolor{LimeGreen}67.39 & 42.66 & 52.59 & 50.62 & 48.95 & 61.21 \\
    SS 3 & 74.88 & 71.75 & \cellcolor{LimeGreen}78.69 & 77.16 & 70.62 & 71.75 & 73.70 \\
    \rowcolor{light-gray}SS 4 & 56.30 & 42.25 & 46.89 & \cellcolor{LimeGreen}59.21 & 44.72 & 54.26 & 52.48 \\
    SS 5 & 53.57 & 48.51 & 45.93 & 62.18 & \cellcolor{LimeGreen}62.83 & 60.21 & 53.07 \\
    \rowcolor{light-gray}SS 6 & & 51.90 & 38.28 & 49.82 & 47.15 & \cellcolor{LimeGreen}55.71 & 54.36 \\
    \bottomrule
  \end{tabu}
\end{sstable}

A more detailed examination of of study site 3’s results shows that the increased accuracy is due to the fact that relatively few of the pixels in the study site are crop pixels (\autoref{table:round1ss3acc}). A higher accuracy thus results because so many pixels were left unclassified as ``other.''

\begin{sstable}
  \centering
  \caption{Round 1 Testing: Study Site 3 Best Accuracy, Using NDVI Data}
  \label{table:round1ss3acc}
  \begin{tabu}{rrrrrrrl}
    \toprule
     & & \multicolumn{4}{c}{\textbf{Reference Data}} & & \\     & & Corn & Soy & Wheat & Other & Total & User Acc. \\    \midrule    \multirow{4}{*}{\rotatebox{90}{\textbf{Classified}}} & Corn & 861 & 133 & 0 & 334 & 1328 & 64.83\% \\     & Soy & 28 & 558 & 13 & 198 & 797 & 70.01\% \\     & Wheat & 2 & 4 & 23 & 37 & 66 & 34.84\% \\     & Other & 884 & 424 & 74 & 6427 & 7809 & 82.30\% \\     & Total & 1775 & 1119 & 110 & 6996 & 10000 &  \\     & Producer Acc. & 48.51\% & 49.87\% & 20.91\% & 91.87\% &  &  \\    \multicolumn{8}{r}{Overall Accuracy: 78.69\%} \\    \multicolumn{8}{r}{Kappa: 0.49} \\
    \bottomrule
  \end{tabu}
\end{sstable}

As for NDVI versus EVI, the results show that NDVI consistently has higher classification accuracies. For this reason, I chose to continue all the following tests using NDVI. However, the EVI results were not far behind, so I am hesitant to conclude that future optimizations could not make EVI-based classifications as or more accurate than NDVI-based classifications. Further testing is required in this regard.

That each study site had its highest accuracy classification using its own references signatures was probably the most significant finding of the test. This result was not wholly unexpected: even if temporal signatures are largely location-independent, there is likely to be some location-specific phenomena influencing the shape of the signature curves. In some cases, it appears that some classifications using other study site reference signatures were of similar accuracy. For instance, the study site 1 NDVI classification using study site 4’s signatures has an accuracy of 60.7 percent, versus 61.1 percent with study site 1's own signatures. However, in all other cases (using signature from study sites 2, 3, 4, 5, and 6), the study site 1 NDVI classification accuracies are about 10 percent lower.

The accuracies of the mean reference signature classifications are generally low. They are never the worst, which may suggest that it is better to average a greater number of pixels together if the representative-ness of the chosen pixels cannot be established. However, this conclusion would support a more selective and refined approach to creating reference signatures: don’t try to average out bad pixels, eliminate them in the first place.

Looking back on the outcomes I outlined above, I actually found none of them completely captured the behavior shown in the results. I did not see relatively consistent classification accuracies independent of the set of reference signatures used, but I also found that the reference signatures from some study sites did well at classifying others. The mean signatures were also somewhat in the middle: generally they did not do terribly well, but sometimes came close. The best interpretation I could make of these results is that, under the right circumstances, reference signatures can be used to classify other areas. However, the cases where the reference signatures were not portable were concerning, as were the low overall accuracy levels. I began to consider that I might not be able to answer the initial testing questions. Instead, I realized I needed to take a step back and better explore what factors affect the classification process.


\section{Round 2 Testing: Eliminating Mixels}
\label{appendix:testing:r2}

\subsection*{Pre-testing Investigation}

I began my Round 2 testing by diving back into my Round 1 results. I wasn’t sure exactly what I needed to test, but I knew my previous results held more clues. For the sake of simplifying my investigation, I decided to focus solely on study site 1 (SS1) for the remainder of my testing. If I could boost the accuracy of its classification, I would identify some of the factors influential in the classification process. I chose SS1 over the others as it has the best variety of crops in which I am interested, and also includes some large non-crop areas of different land cover types. This mix seemed to offer the best testing environment.

First, I studied the classification results for SS1 from Round 1 produced with its own reference signatures (\autoref{fig:ss1r1class}). I noticed that the major patterns generally matched the CDL fairly well. If the classification \textit{looks} correct, however, where were the errors? Obviously there should be many incorrect pixels, as this classification only had an accuracy of 61.1 percent. Yet they weren’t obvious at first glance.

\begin{ssfigure}
  \centering
  \begin{subfigure}[t]{.475\textwidth}
    \includegraphics[width=\textwidth]{Graphics/Testing/clip1_MODIS_CDL.pdf}
    \caption{The 2012 CDL for SS1}
    \label{subfig:ss1r1CDL}
  \end{subfigure}
  \quad
  \begin{subfigure}[t]{.475\textwidth}
    \includegraphics[width=\textwidth]{Graphics/Testing/clip1_MODIS_round1.pdf}
    \caption{Round 1 classification using SS1 signatures}
    \label{subfig:ss1r1class}
  \end{subfigure}
  \\
  \vspace{.25in}
  \begin{subfigure}[b]{.475\textwidth}
    \includegraphics[width=\textwidth]{Graphics/Testing/clip1_MODIS_round1_correct.pdf}
    \caption{Correctly classified pixels\break~Incorrect pixels are black}
    \label{subfig:ss1r1correct}
  \end{subfigure}
  \quad
  \begin{subfigure}[b]{.475\textwidth}
    \includegraphics[width=\textwidth]{Graphics/Testing/clip1_MODIS_round1_incorrect.pdf}
    \caption{Incorrectly classified pixels\break~Correct pixels are black}
    \label{subfig:ss1r1incorrect}
  \end{subfigure}
  \caption{Round 1 Testing: Study Site 1 Classification.}
  \label{fig:ss1r1class}
  \medskip
  \small
  Corn pixels are shown in yellow, while soy pixels are dark green, and winter wheat pixels are brown. Notice that the classification in (b) looks to do a decent job of capturing where the different crops occur in (a), but (c) and (d) show numerous errors on the edges of fields, and some class confusion.
\end{ssfigure}

To find these incorrect pixels, I created an image with of the correct pixels and an image of the incorrect pixels (\autoref{subfig:ss1r1correct} and \autoref{subfig:ss1r1incorrect}). In the former, I noticed that many of the black incorrect pixels seemed to fall on the edges of fields. In the latter, I noticed some class confusion, a finding reinforced by the confusion matrix for this classification (\autoref{table:ss1r1acc}).

\begin{sstable}
  \centering
  \caption{Round 1 Testing: Study Site 1 Best Accuracy, Using NDVI Data}
  \label{table:ss1r1acc}
  \begin{tabu}{rrrrrrrl}
    \toprule
     & & \multicolumn{4}{c}{\textbf{Reference Data}} & & \\ &  & Corn & Soy & Wheat & Other & Total & User Acc. \\\midrule\multirow{4}{*}{\rotatebox{90}{\textbf{Classified}}} & Corn & 657 & 299 & 16 & 174 & 1146 & 57.33\% \\ & Soy & 121 & 863 & 27 & 268 & 1279 & 67.47\% \\ & Wheat & 137 & 95 & 1250 & 379 & 1861 & 67.17\% \\ & Other & 688 & 803 & 885 & 3338 & 5714 & 58.42\% \\ & Total & 1603 & 2060 & 2178 & 4159 & 10000 &  \\ & Producer Acc. & 40.99\% & 41.89\% & 57.39\% & 80.26\% &  &  \\\multicolumn{8}{r}{Overall Accuracy: 61.08\%} \\\multicolumn{8}{r}{Kappa: 0.43} \\    \bottomrule
  \end{tabu}
\end{sstable}

The class confusion seemed to be a problem, but how to begin to remedy it was not obvious. That so many border pixels were incorrectly classified, conversely, suggested that this classification method struggles when pixels have more than one land cover. Such ``mixed-pixels'' are often termed ``mixels.''

The problem with mixels is that each land cover in a mixel has a different temporal signature. The different signatures are aggregated at the pixel level and become mixed, creating a new signature representing that specific mixture of individual signatures. This problem is not unique to my temporal data; all raster land cover data have mixels. Users of spectral data even have spectral unmixing tools to extract subpixel spectral information.

In my case, the large size of the MODIS pixels increases their possible effect on classification accuracy. Many different land covers can be included within a 231-square-meter area. The large pixel size also means that a much greater percentage of the study area is composed of mixels than if a smaller pixel were used.

For these reasons, I hypothesized the low classification accuracy was partially because mixels could not be processed accurately by the fit algorithm. In other words, if two (or more) crops are mixed within a pixel, the curve of that pixel’s values will be a blend of both crops’ phenological signatures, and neither of the two crops' references signatures will fit the pixel well. Moreover, the crop which may occupy the majority of the pixel's area may not be the largest contributor a pixel’s values. For instance, a mature crop may drive up the VI values for a pixel, even if that crop is in the minority of the pixel. Consequently, I determined I needed to find a way to removed mixels from the classification.


\subsection*{The Testing Process}

The first step to removing mixels was to create a vector grid match the MODIS TSI of SS1. Then, I converted the 30-meter CDL raster ground truth dataset to vector polygons. Intersecting these CDL features with the TSI pixel polygons combined the geometries of both. Consequently, pixel polygons that fell within a continuous land cover area were undivided, but mixels were split up into multiple polygons by each land cover class within them. To find all the non-mixels in the study area, I selected all features with an area close to that of a full MODIS pixel from the resulting polygons. Specifically, I chose to select all features greater than or equal to 53,000 square meters in area (or 98 percent of a full MODIS pixel, which is 53,824 square meters). I also manually added two sorghum pixel features that were not selected via this process because of the low number of sorghum pixels retained. The result, shown in \autoref{fig:ss1purepx}, was 1,359 features selected. These features can be thought to represent MODIS pixels that have ``pure'' signatures: each pixel has only one land cover contributing to its temporal signature, so each should be representative of its class.

\begin{ssfigure}
  \centering
  \includegraphics[width=.9\textwidth]{Graphics/Testing/clip1_30mCDL_pure_pixels.pdf}
  \caption{Pure Pixels Delineated in Study Site 1}
  \label{fig:ss1purepx}
\end{ssfigure}

To allow me to classify only these pure pixels, I found the centroid of each of the selected pixel polygon features. The subset option of the Find Fit Tool uses a shapefile of point features to create a list of pixel coordinates in the TSI to process. Only these 1,359 pixels were fit with the reference signatures. All the other pixels were assigned NoData (-3000 is the MODIS NoData value). Each of the other classification steps were the same as in Round 1, except the NoData values in the RMSE rasters were ignored by the Classify tool when considering accuracy.


\subsection*{Round 2 Results and Discussion}

The pure-pixel-only classification is shown in \cref{fig:ss1r2class}, and its confusion matrix is \autoref{table:ss1r2acc}. The confusion matrix shows the vast majority of errors are errors of commission in the corn class. Almost one third of the soy pixels and close to half of the ``other'' pixels are classified as corn. A map with the incorrect pixels highlighted is shown in \cref{fig:ss1r2class_correct}. That corn and soy have similar signatures may suggest that those crops will always have some confusion. In other words, because the similar shape and close maximum dates of early soy and late corn, they may not be differentiable to the fit algorithm. However, if this hypothesis were true, I expected to see the confusion in both directions, with a greater number of corn pixels wrongly classified as soy. Additionally, this hypothesis does not explain why so many ``other'' pixels were classified as corn. Thus, I began to consider that the reference signatures I created might not be accurate.

\begin{ssfigure}
  \centering
  \includegraphics[width=\textwidth]{Graphics/Testing/clip1_MODIS_round2.pdf}
  \caption{Round 2 Testing: Classification of Study Site 1 Pure Pixels}
  \label{fig:ss1r2class}
  The classified pixels are outlined in black. Green pixels are soy, yellow are corn, and brown are winter wheat. The outlined white pixels were classified as other.
\end{ssfigure}


\begin{ssfigure}
  \centering
  \includegraphics[width=\textwidth]{Graphics/Testing/clip1_MODIS_round2_correct.pdf}
  \caption{Round 2 Testing: Correct Pixels in Study Site 1 Pure Pixel Classification}
  \label{fig:ss1r2class_correct}
  \medskip
  \small
  This is the same as \cref{fig:ss1r2class}, except all the incorrectly classified pixels are colored black to distinguish them from the correct pixels.
\end{ssfigure}

\begin{sstable}
  \centering
  \caption{Round 2 Testing: Study Site 1 NDVI Classification of Pure Pixels}
  \label{table:ss1r2acc}
  \begin{tabu}{rrrrrrrl}
    \toprule
     & & \multicolumn{4}{c}{\textbf{Reference Data}} & & \\     & & Corn & Soy & Wheat & Other & Total & User Acc. \\    \midrule    \multirow{4}{*}{\rotatebox{90}{\textbf{Classified}}} & Corn & 358 & 108 & 3 & 79 & 548 & 65.34\% \\     & Soy & 10 & 236 & 0 & 11 & 257 & 91.83\% \\     & Wheat & 36 & 6 & 348 & 29 & 419 & 83.05\% \\     & Other & 10 & 4 & 20 & 101 & 135 & 74.81\% \\     & Total & 414 & 354 & 371 & 220 & 1359 &  \\     & Producer Acc. & 86.47\% & 66.67\% & 93.80\% & 45.91\% &  &  \\    \multicolumn{8}{r}{Overall Accuracy: 76.75\%} \\
    \multicolumn{8}{r}{Kappa: 0.68} \\  
    \bottomrule
  \end{tabu}
\end{sstable}

To investigate this idea, I plotted the reference signatures I extracted from SS1 (\autoref{fig:KSfirstsigs}). Comparing the plotted signatures to those shown in phenological classification literature  revealed numerous discrepancies \autocites{wardlow2002discriminating}{wardlow2005state-level}{wardlow2007analysis}{wardlow2008large-area}{masialeti2010a-comparative}. To determine the cause of the differences, I plotted the signatures of the pixels sampled to generate each of the reference signatures (shown in \autoref{fig:KSsoyfirstsigs} through \autoref{fig:KSwheatsoyfirstsigs}). Some of these plots showed high variance between the sampled pixels and strange early and late season peaks inconsistent with the accepted temporal signatures for these crops. For example, one abnormally-low valued soy pixel was responsible for pulling down the peak of the soy reference signature. Corn, had extremely variable pixel signatures, which complete mangled its reference signature. Sorghum, among other problems, had very high early season peaks (I have since realized this is likely due to wheat/sorghum double cropping misclassified in the CDL). To correct these and other problems, I realized I needed to change my strategy for identifying the TSI pixels to sample for the reference signatures. Rather than simply finding pure pixels, I needed to ensure the signatures of sampled pixels were representative of the expected crop signature.

\begin{ssfigure}
  \centering
  \input{plots/firstsigsKS.pgf}
  \caption{Crop Reference Signatures Extracted from Study Site 1}
  \label{fig:KSfirstsigs}
\end{ssfigure}

\begin{ssfigure}
  \centering
  \input{plots/firstsoyKS.pgf}
  \caption{Soy Sampled Pixel Signatures and Mean Signature}
    \label{fig:KSsoyfirstsigs}
\end{ssfigure}

\begin{ssfigure}
  \centering
  \input{plots/firstcornKS.pgf}
  \caption{Corn Sampled Pixel Signatures and Mean Signature}
    \label{fig:KScornfirstsigs}
\end{ssfigure}

\begin{ssfigure}
  \centering
  \input{plots/firstsorghumKS.pgf}
  \caption{Sorghum Sampled Pixel Signatures and Mean Signature}
    \label{fig:KSsorghumfirstsigs}
\end{ssfigure}

\begin{ssfigure}
  \centering
  \input{plots/firstwheatKS.pgf}
  \caption{Winter Wheat Sampled Pixel Signatures and Mean Signature}
    \label{fig:KSwheatfirstsigs}
\end{ssfigure}

\begin{ssfigure}
  \centering
  \input{plots/firstwheatsoyKS.pgf}
  \caption[Winter Wheat/Soy Double Crop Sampled Pixel Signatures and Mean Signature]{Winter Wheat/Soy Double Crop Sampled Pixel Signatures\\~and Mean Signature}
    \label{fig:KSwheatsoyfirstsigs}
\end{ssfigure}

\clearpage

\section{Round 3 Testing: Refining the Reference Signatures}
\label{appendix:testing:r3}

The image analysis software ENVI \autocite{envi5.0} has a plotting tool that allows the user to interactively select pixels in a multi band raster and view plots of the pixel values. This tool proved perfect for refining the reference curves. Using it, I could ensure I only sampled crop pixels whose values appeared to match the expected crop signature. The new sample points are shown alongside the old points in \autoref{fig:newpoints}. \Crefrange{fig:KSsoyrefinedsigs}{fig:KSwheatsoyrefinedsigs} shows the individual signatures of the new sampled pixels for each crop, and the refined reference signatures made from those new sample pixels. In each figure, the original reference signature is shown in gray to illustrate the change. The new soy and corn reference signatures peak higher; soy exhibits its distinctive bell-shaped peak, while corn peaks early, remaining at high VI values before tapering off quickly at the end of its season. The new sorghum signature displays none of the early season peaks, and is much rounder compared to the jaggedness before. Unlike the others, however, the new winter wheat and winter wheat/soy double crop signatures do not differ greatly from the originals, but the originals did not exhibit many issues. \autoref{fig:KSenvisigs} is combined plot of all the new reference signatures so they can be compared to one another.

\begin{ssfigure}
  \centering
  \includegraphics[width=0.7\textwidth]{Graphics/Testing/clip1_MODIS_CDL_smpl_old_wnew.pdf}
  \caption{Points Marking New Pixels Sampled to Create New Reference Signatures}
  \label{fig:newpoints}
  \medskip
  \small
  The blue points represent the new pixels sampled to create the new crop reference signatures. The red points are the old points used to create the original crop signatures. Some of the original pixels had good signatures and were used again for the refined signatures.
\end{ssfigure}


\begin{ssfigure}
  \centering
  \input{plots/refinedsoyKS.pgf}
  \caption{Refined Soy Sampled Pixel Signatures and Mean Signature}
    \label{fig:KSsoyrefinedsigs}
\end{ssfigure}

\begin{ssfigure}
  \centering
  \input{plots/refinedcornKS.pgf}
  \caption{Refined Corn Sampled Pixel Signatures and Mean Signature}
    \label{fig:KScornrefinedsigs}
\end{ssfigure}

\begin{ssfigure}
  \centering
  \input{plots/refinedsorghumKS.pgf}
  \caption{Refined Sorghum Sampled Pixel Signatures and Mean Signature}
    \label{fig:KSsorghumrefinedsigs}
\end{ssfigure}

\begin{ssfigure}
  \centering
  \input{plots/refinedwheatKS.pgf}
  \caption{Refined Winter Wheat Sampled Pixel Signatures and Mean Signature}
    \label{fig:KSwheatrefinedsigs}
\end{ssfigure}

\begin{ssfigure}
  \centering
  \input{plots/refinedwheatsoyKS.pgf}
  \caption[Refined Winter Wheat/Soy Double Crop Sampled Pixel Signatures and Mean Signature]{Refined Winter Wheat/Soy Double Crop Sampled Pixel\\~Signatures and Mean Signature}
    \label{fig:KSwheatsoyrefinedsigs}
\end{ssfigure}

\begin{ssfigure}
  \centering
  \input{plots/envisigsKS.pgf}
  \caption{Refined Crop Signatures Extracted from the 2012 Kansas TSI}
  \label{fig:KSenvisigs}
\end{ssfigure}

\clearpage

\subsection*{Round 3 Results and Discussion}

%[LINK TO THIS CLASSIFICATION: /Users/phoetrymaster/Documents/School/Geography/Thesis/Data/MODIS_KANSAS_2007-2012/reprojected/Classified/test1_envicurves/fullpxonly/clip1refs/classification_2014-07-29_1446/]

To my surprise, refining the reference signatures actually reduced the classification accuracy to 65.8 percent (\autoref{table:ss1r3acc}). I did notice almost all of the ``other'' pixels were correctly classified, but also that, in contrast to the Round 2 testing, errors of omission were primarily responsible for the decrease in accuracy. That is, many corn pixels in the study were classified as other. The confusion between corn and soy also remained.

\begin{sstable}
  \centering
  \caption[Round 3 Testing: Study Site 1 NDVI Classification of Pure Pixels Using Refined Reference Signatures]{Round 3 Testing: Study Site 1 NDVI Classification of Pure Pixels\\~Using Refined Reference Signatures}
  \label{table:ss1r3acc}
  \begin{tabu}{rrrrrrrl}
    \toprule
     & & \multicolumn{4}{c}{\textbf{Reference Data}} & & \\     &  & Corn & Soy & Wheat & Other & Total & User Acc. \\    \midrule    \multirow{4}{*}{\rotatebox{90}{\textbf{Classified}}} & Corn & 201 & 117 & 0 & 7 & 325 & 61.85\% \\     & Soy & 1 & 196 & 0 & 16 & 213 & 92.02\% \\     & Wheat & 41 & 9 & 340 & 40 & 430 & 79.07\% \\     & Other & 171 & 32 & 31 & 157 & 391 & 40.15\% \\     & Total & 414 & 354 & 371 & 220 & 1359 &  \\     & Producer Acc. & 48.55\% & 55.37\% & 91.64\% & 71.36\% &  &  \\    \multicolumn{8}{r}{Overall Accuracy: 65.78\%} \\    \multicolumn{8}{r}{Kappa: 0.55} \\
    \bottomrule
  \end{tabu}
\end{sstable}

To further understand what had happened, I plotted the signature of every incorrectly classified pixel in the study site. A small selection of strange signatures from the many pixels labeled as corn by the CDL that were not accurately classified is shown in \cref{fig:weirdcorns}. Looking through the plots, I began to notice some patterns were repeated. Among the signatures in \cref{fig:weirdcorns} there are many similarities and differences. Pixel signatures 1, 2, 3, and 5 all share an early season peak, but tend to diverge after the first dip in values around DOY 113. Pixel 4's signature does not appear distinctly different than the corn reference signature up until DOY 257, when it begins to climb steeply to a second peak.

\begin{ssfigure}
  \centering
  \input{plots/weirdsigs2.pgf}
  \caption{Selected Examples of Strange Signatures from Pixels Labeled Corn in the CDL}
  \label{fig:weirdcorns}
  \medskip
  \small
  Each of these signatures was extracted from a pixel labeled corn by the CDL that was not accurately classified in the Round 3 testing. The signatures of pixels 1, 2, 3, and 5 share features on a few key dates, but diverge on others. None of these four is remotely similar in appearance to corn. In fact, the two peaks, around DOY 97 and DOY 161, suggest some sort of double crop rotation. The only signature similar to corn is that of pixel 4, which only departs from the corn reference signature around DOY 257. After that date it sharply increases to an end-of-year peak, suggestive of winter wheat planting for the following year.
\end{ssfigure}

Comparing the pixel coordinates of all the incorrect pixel plots, I found many adjacent pixels with similar plot shapes. This finding provides evidence that all pixels in a field generally have the same signature and supports the overall theory behind this method. That is, if all plants of a crop grow under the same conditions, they should have the same temporal signatures.

However, the many different signatures I found within each crop class of the CDL is particularly troublesome. Assuming the truthfulness of the CDL, this suggests that each crop has multiple temporal signatures. Not only does such reasoning conflict with previous research into phenological classification, but is illogical considering the typical growth cycle for crops like corn and soy. The same crops within close proximity should be exposed to essentially equal growing conditions. Variations in planting date may account for slight differences in the temporal signatures. The use of irrigation or application of pesticides, fertilizers, or herbicides may also impart slight disparities between signatures \autocites{wardlow2005state-level}{wardlow2007analysis}{wardlow2008large-area}{sakamoto2010a-two-step}. However, none of these variables would likely be accountable for the vastly different crop signatures observed.

To get an expert opinion, on \formatdate{31}{5}{2014} I e-mailed a few of the signatures plots of incorrectly classified pixels, labeled as corn in the CDL (similar to pixels 1, 2, 3, and 5 in \cref{fig:weirdcorns}, to Dana Peterson, a research assistant at the Kansas Applied Remote Sensing Program of the Kansas Biological Survey at University of Kansas. She and her colleagues have significant experience working with phenological classifications made using MODIS VI data and the Kansas CDL. Her reply of \formatdate{1}{6}{2014} confirmed that the signature types I sent is likely attributable to double cropping, and that the CDL is not correct in these cases.

The remaining incorrectly classified pixels that did not have the double crop appearance were, predominately, similar to pixel 4 in \cref{fig:weirdcorns}. These pixel signatures all had an unusual bump in value at the end of the year. Reviewing typical crop signatures plots in the phenological classification literature made me think the end-of-year increase in NDVI of many incorrectly classified pixel may be related to winter wheat planting for the next year’s growing season.

To investigate this idea, I used the U.S. Geological Survey Landsat Look online imagery viewer to identify some of these incorrectly classified fields and see how they appeared through time. The fields with the increase in values at the end of the year featured vegetation throughout the winter, and had significant vegetation growth early in the following year, consistent with winter wheat. Moreover, an analysis using the CDL for 2013 showed 172 of the incorrectly classified pixels were planted with winter wheat in fall 2012.


\section{Round 4 Testing: Different Time Ranges}
\label{appendix:testing:r4}

\subsection*{Deriving the Date Ranges to Test}

As many of the pixels this winter wheat bump did not fit the reference signatures within the utilized RMSE thresholds, I wondered what would happen if I changed the date range of the TSI. Instead of beginning and ending the TSI at the start of January, I decided to create a TSI that would include the end-of-year winter wheat bump from 2011 and end before the bump in 2012. Specifically, I selected images from 2011 DOY 305 through 2012 DOY 289.

%Don't think I need the map...
%\missingfigure{CREATE A MAP OF INCORRECTLY CLASSIFIED SHOWING 2013 WINTER WHEAT PIXELS} % SEE /Users/phoetrymaster/Documents/School/Geography/Thesis/Data/MODIS_KANSAS_2007-2012/reprojected/Classified/test1_envicurves/fullpxonly/clip1refs/classification_2014-07-29_1446/wheat2013.tif].

As by this point I had also returned from my field work in Argentina, I had a bit better idea of how my processing procedure would have to change to classify the Pellegrini imagery. In talking with the locals on Pellegrini, I found that wheat was just one of a number of different grains that were grown in the winter dry season. Moreover, given my visit was in the middle of the summer growing season, I did not have any good way to verify where nor what types of winter crops were grown. To compound the problem, I also learned that, due to the length and flexibility of the summer wet season in Pellegrini, farmers were not limited to double cropping only late summer crops with a winter crop. Instead, any summer crop could be grown with a winter crop. As I only extracted one double crop signature from the Kansas data, the winter wheat and soy signature, I realized I would not be able to use my Kansas signatures to classify an entire agricultural year in Argentina. Rather, I could only classify summer crops. Consequently, I decided to classify SS1 during only the spring and summer months, using MODIS imagery from DOY 97 through DOY 273.

\subsection*{Classification Procedures}

The only difference in the processing procedure for these two classification as compared to the previous procedure in Round 3 was that I selected the desired MODIS .hdf files covering each of the date ranges and placed those files in two folders. I then used those folders as the directories for the MODIS .hdf sources when building the two TSIs. I used the same reference signatures as derived in Round 3, and only classified the pure pixels identified in Round 2.

\subsection*{Round 4 Results and Discussion}

Despite accounting for the winter wheat planting for the following year, my 2011--2012 classification of corn, soy, and winter wheat was only marginally better than the full 2012 classification. The overall accuracy was only 68.1 percent, as shown in \cref{table:r4ss1acc2011-2012a}. Looking at the classification results, it appears that a few of the winter wheat 2013 pixels were properly classified this time, but many remained incorrect. Evidenced by the percent accuracy, most improvements were offset by pixels now unable to be accurately classified. Given that I knew I would be focusing only on summer crops in Argentina, I did not further investigate why this was the result.

\begin{sstable}
  \centering
  \caption[Round 4 Testing: Study Site 1 NDVI Classification of Pure Pixels Using Refined Reference Signatures, 2011 DOY 305 Through 2012 DOY 289]{Round 4 Testing: Study Site 1 NDVI Classification of Pure Pixels Using\\~Refined Reference Signatures, 2011 DOY 305 Through 2012 DOY 289}
  \label{table:r4ss1acc2011-2012a}
  \begin{tabu}{rrrrrrrl}
    \toprule
     & & \multicolumn{4}{c}{\textbf{Reference Data}} & & \\     &  & Corn & Soy & Wheat & Other & Total & User Acc. \\    \midrule    \multirow{4}{*}{\rotatebox{90}{\textbf{Classified}}} & Corn & 241 & 117 & 0 & 7 & 365 & 66.03\% \\     & Soy & 16 & 202 & 3 & 33 & 254 & 79.53\% \\     & Wheat & 97 & 4 & 333 & 31 & 465 & 71.61\% \\     & Other & 60 & 31 & 35 & 149 & 275 & 54.18\% \\     & Total & 414 & 354 & 371 & 220 & 1359 &  \\     & Producer Acc. & 58.21\% & 57.06\% & 89.76\% & 67.73\% &  &  \\    \multicolumn{8}{r}{Overall Accuracy: 68.06\%} \\    \multicolumn{8}{r}{Kappa: 0.57} \\
    \bottomrule
  \end{tabu}
\end{sstable}

The 2012 summer-only classification fared slightly better. Given the date range, I only classified the three main summer crops---corn, soy, and sorghum---finding an accuracy of 75.1 percent. Looking at the classification and confusion matrix, I realized much of the error was due to confusion between soy and sorghum (\cref{table:r4ss1acc2012a}).

%[SEE /Users/phoetrymaster/Documents/School/Geography/Thesis/Data/MODIS_KANSAS_2007-2012/reprojected/Summer2012/multidate_image/classification_2014-08-09_1716_5/2014-08-09_1716_2012clip1.txt]

\begin{sstable}
  \centering
  \caption[Round 4 Testing: Study Site 1 NDVI Classification of Pure Pixels Using Refined Reference Signatures, 2012 DOY 97 Through DOY 273]{Round 4 Testing: Study Site 1 NDVI Classification of Pure Pixels Using \\~Refined Reference Signatures, 2012 DOY 97 Through DOY 273}
  \label{table:r4ss1acc2012a}
  \begin{tabu}{rrrrrrrl}
    \toprule
     & & \multicolumn{4}{c}{\textbf{Reference Data}} & & \\     &  & Corn & Soy & Sorghum & Other & Total & User Acc. \\    \midrule    \multirow{4}{*}{\rotatebox{90}{\textbf{Classified}}} & Corn & 394 & 120 & 0 & 27 & 541 & 72.83\% \\     & Soy & 0 & 76 & 4 & 5 & 85 & 89.41\% \\     & Sorghum & 6 & 135 & 12 & 3 & 156 & 7.69\% \\     & Other & 14 & 23 & 2 & 538 & 577 & 93.24\% \\     & Total & 414 & 354 & 18 & 573 & 1359 &  \\     & Producer Acc. & 95.17\% & 21.47\% & 66.67\% & 93.89\% &  &  \\    \multicolumn{8}{r}{Overall Accuracy: 75.06\%} \\    \multicolumn{8}{r}{Kappa: 0.63} \\
    \bottomrule
  \end{tabu}
\end{sstable}

Considering that only 18 pixels in the study area were sorghum, I tried classifying the data again without the sorghum signature, classifying just corn and soy. However, this merely traded confusion between sorghum and soy for confusion between corn and soy, and the overall accuracy was essentially unchanged at 75.3 percent.

%[SEE /Users/phoetrymaster/Documents/School/Geography/Thesis/Data/MODIS_KANSAS_2007-2012/reprojected/Summer2012/multidate_image/classification_2014-08-09_1831_11/2014-08-09_1831_2012clip1.txt]

In both summer-only classifications, I did find the accuracy increased over the entire year classifications. However, I began to realize that these results were not actually comparable to the previous classification results. Before I had always been finding winter wheat, corn, and soy. However, these results classified corn, soy, and sorghum. To allow a direct comparison between the 2011-2012 and the summer 2012 classifications, I reclassified the 2011--2012 RMSE rasters, looking for corn, soy, and sorghum without wheat. To my surprise, I found the overall accuracy to be more or less the same as classifying just the summer months. Looking more closely at the confusion matrix of this classification (\cref{table:r4ss1acc2011-2012b} and comparing it with that of the summer 2012 classification (\cref{table:r4ss1acc2012a} reveals a few discrepancies. In the summer only classification, one can see a significant degree of confusion between sorghum and soy that is not present in the 2011-2012 classification. However, the producer accuracy for soy in the 2011-2012 classification is only marginally better, as soy is instead confused with corn. Looking through the other statistics in the tables, seeming any advantage of one classification over the other is balanced by other inaccuracies. In other words, without a specific reason to choose one over the other, they perform equally.

%[SEE /Users/phoetrymaster/Documents/School/Geography/Thesis/Data/MODIS_KANSAS_2007-2012/reprojected/2011-2012/multidate_image_1/classification_2014-08-09_1840_5/2014-08-09_1840_2012clip1.txt]

\begin{sstable}
  \centering
  \caption[Round 4 Testing: Study Site 1 NDVI Classification of Pure Corn, Soy, and Sorghum Pixels Using Refined Reference Signatures, 2011 DOY 305 Through 2012 DOY 289]{Round 4 Testing: Study Site 1 NDVI Classification of Pure\\~Corn, Soy, and Sorghum Pixels Using Refined Reference Signatures,\\~2011 DOY 305 Through 2012 DOY 289}
  \label{table:r4ss1acc2011-2012b}
  \begin{tabu}{rrrrrrrl}
    \toprule     & & \multicolumn{4}{c}{\textbf{Reference Data}} & & \\     &  & Corn & Soy & Sorghum & Other & Total & User Acc. \\    \midrule    \multirow{4}{*}{\rotatebox{90}{\textbf{Classified}}} & Corn & 389 & 211 & 7 & 69 & 676 & 57.54\% \\     & Soy & 0 & 124 & 3 & 2 & 129 & 96.12\% \\     & Sorghum & 0 & 4 & 6 & 0 & 10 & 60.00\% \\     & Other & 25 & 15 & 2 & 502 & 544 & 92.28\% \\     & Total & 414 & 354 & 18 & 573 & 1359 &  \\     & Producer Acc. & 93.96\% & 35.03\% & 33.33\% & 87.61\% &  &  \\    \multicolumn{8}{r}{Overall Accuracy: 75.13\%} \\    \multicolumn{8}{r}{Kappa: 0.62} \\
    \bottomrule
  \end{tabu}
\end{sstable}

This result validates that restricting the classification to only the summer months does not have a negative effect when classifying summer crops. In light of this result, and my experience in Argentina, I decided to use the summer-only approach for all subsequent testing.


\section{Round 5: A Last Ditch Effort to Match the CDL}

In attempt to create a classification to match the CDL as best as possible, I used a cluster analysis of the pixels of each of the crops to try to isolate the different signatures I previously identified. This classification process and the subsequent results are those presented in the body of this thesis. To read about the methods, see \cref{methods} on \cpageref{methods}. The results are presented in \cref{results:classifications} on \cpageref{results:classifications}, so I will only briefly summarize my findings here. 

Using the clusters for did in fact increase the classification accuracy to 84.4 percent. A map of the classification, \cref{map:KSclassification}, is on \cpageref{map:KSclassification}. The confusion matrix is \cref{table:ksresults} on \cpageref{table:ksresults}. Sorghum was not well classified, which may have something to do with the date range used (see the full discussion of the results in \cref{discussion:ksclassification} on \cpageref{discussion:ksclassification}). However, it is also possible that the sorghum pixels were not accurately identified by the CDL. The CDL does not have clearly-delineated sorghum fields, but rather a mix of soy and sorghum. Thus, it appears that the USDA’s classification method is unable to accurately differentiate between soy and sorghum in some cases. Consequently, sorghum pixels may have been included in the soy clusters, and vise versa. This would mix the temporal signatures and cause confusion in the results. 

Of course, the soy and sorghum signatures are rather similar in appearance, and it might be that my method is unable to distinguish them from one another. Even if the CDL has confusion between the two, this may be case. More testing is necessary to determine what might be happing.

%[See /Users/phoetrymaster/Documents/School/Geography/Thesis/Data/MODIS_KANSAS_2007-2012/reprojected/Summer2012/kmeansTesting_2/classification_2014-07-11_1117 AND classification_2014-07-11_1639/]

Also mentioned in the discussion, specifically \cref{discussion:kssigs} on \cpageref{discussion:kssigs}, one of the soy signatures had a strange, non-soy-like appearance (Soy\_1 in \cref{fig:KScropsigs} on \cpageref{fig:KScropsigs}. I decided to re-run the classifier as before, except I omitted this soy signature. The accuracy did drop, to 81.5 percent, but when I visually analyzed the classification I could see that all the previous errors where non-crop pixels were classified as soy were no longer present. I interpret these results to mean one of two things: (1) some soy fields have signatures quite similar to grassland and pasture areas, or (2) the CDL inaccurately classifies some grassland or pasture as soy. From my understanding of crop phenologies, and given my experience looking at crop signatures, I believe the latter is more likely.

%[See /Users/phoetrymaster/Documents/School/Geography/Thesis/Data/MODIS_KANSAS_2007-2012/reprojected/Summer2012/kmeansTesting_2/classification_2014-07-12_1612/]

\section{Discussion and Conclusions}

Are the results still rather low because the CDL has class confusion? That is, might my inaccuracy be compounded because of inaccuracies in the CDL that my classification methods will never recreate? I must posit that my method might actually be more accurate than I can test given the problems with the CDL. However, even if the accuracy of the CDL is 90 percent, as is published, what is the a reasonable accuracy for me to achieve? If my classification were 100 percent accurate, comparing it to the CDL would only result in 90 percent accuracy. Thus, a classification of 80 percent accuracy may indeed be higher relative to the actual ground conditions. Only further testing with confirmed ground truth can really say.